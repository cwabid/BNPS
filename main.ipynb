{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:134.0) Gecko/20100101 Firefox/134.0\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"iframe\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "        \"Priority\": \"u=4\",\n",
    "        \"TE\": \"trailers\"\n",
    "    })\n",
    "    return session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(session, url):\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.content        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_navigation_urls(html_content, base_url=\"https://www.thedailystar.net\"):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        unique_links = {}\n",
    "\n",
    "        for ul in soup.find_all(\"ul\", class_=\"sub-category-menu\"):\n",
    "            for li in ul.find_all(\"li\"):\n",
    "                a_tag = li.find(\"a\")\n",
    "                if a_tag:\n",
    "                    title = a_tag.text.strip()\n",
    "                    link = a_tag.get(\"href\", \"\")\n",
    "                    if link.startswith(\"/\"):\n",
    "                        link = base_url + link\n",
    "                    \n",
    "                    unique_links[link] = title\n",
    "\n",
    "        return [{\"title\": title, \"url\": link} for link, title in unique_links.items()]\n",
    "    except Exception:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news_links(html_content, base_url=\"https://www.thedailystar.net\"):\n",
    "    try:\n",
    "        if not html_content:\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        news_links = {}\n",
    "\n",
    "        for h3 in soup.find_all(\"h3\", class_=[\"title\", \"title fs-20\"]):  \n",
    "            a_tag = h3.find(\"a\", href=True)\n",
    "            if a_tag:\n",
    "                title = a_tag.get_text(strip=True)\n",
    "                link = a_tag[\"href\"]\n",
    "                \n",
    "                # Ensure full URL\n",
    "                if link.startswith(\"/\"):\n",
    "                    link = base_url + link\n",
    "\n",
    "                # Store unique titles\n",
    "                news_links[link] = title\n",
    "\n",
    "        return [{\"title\": title, \"url\": url} for url, title in news_links.items()]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting news links: {e}\")\n",
    "        return []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_news_content(html_content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        news_section = soup.find(\"div\", class_=\"pb-20 clearfix\")\n",
    "\n",
    "        if not news_section:\n",
    "            return \"\"\n",
    "\n",
    "        paragraphs = [p.get_text(strip=True) for p in news_section.find_all(\"p\")]\n",
    "        return \" \".join(paragraphs)\n",
    "    except Exception:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_unique_news(html_content):\n",
    "    try:\n",
    "        if not html_content:\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all articles\n",
    "        articles = soup.find_all(\"article\", class_=\"article-section\")\n",
    "\n",
    "        news_data = {}\n",
    "        \n",
    "        for article in articles:\n",
    "            # Extract the news title\n",
    "            title_tag = article.find(\"h1\", class_=\"article-title\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "\n",
    "            # Extract the news content\n",
    "            content_section = article.find(\"div\", class_=\"pb-20 clearfix\")\n",
    "            paragraphs = [p.get_text(strip=True) for p in content_section.find_all(\"p\")] if content_section else []\n",
    "            content = \" \".join(paragraphs) if paragraphs else \"No Content\"\n",
    "\n",
    "            # Add to dictionary to ensure uniqueness (title as key)\n",
    "            news_data[title] = content\n",
    "\n",
    "        # Convert back to list of dictionaries\n",
    "        return [{\"title\": title, \"content\": content} for title, content in news_data.items()]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting news: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    session = create_session()\n",
    "    url = \"https://www.thedailystar.net/news\"\n",
    "\n",
    "    html_content = make_request(session, url)\n",
    "    urls = extract_unique_navigation_urls(html_content,)\n",
    "    #print(urls)\n",
    "    for page in urls:\n",
    "        print(f\"Scraping {page['title']}\")\n",
    "        new_page = make_request(session,page[\"url\"])\n",
    "        news_titles = extract_news_links(new_page,page[\"title\"].lower())\n",
    "        print(len(news_titles))\n",
    "        for news in news_titles:\n",
    "            #print(news[\"title\"])\n",
    "            #contents = make_request(session, news[\"url\"])\n",
    "            #description = extract_unique_news(contents)\n",
    "            #for news in description:\n",
    "                # print(\"üì∞ Title:\", news[\"title\"])\n",
    "                # print(\"üìÑ Content:\", news[\"content\"][:500])  # Print first 500 characters\n",
    "                # print(\"-\" * 80)\n",
    "                pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Bangladesh\n",
      "21\n",
      "Scraping Investigative Stories\n",
      "18\n",
      "Scraping Asia\n",
      "21\n",
      "Scraping World\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
